---
title: "Functional Reproducibility"
subtitle: "Engineering reproducibility in the face of entropy"
author: "Robin Gower"
abstract: Presented at Berlin R User Group March-2024
format: revealjs
#format:
#  html:
#    standalone: true
#    embed-resources: true
#    header-includes: |
#      <link rel="preconnect" href="https://fonts.googleapis.com">
#      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
#      <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro&display=swap" rel="stylesheet"> 
#    title-block-banner: true
editor: source
execute:
  echo: true
  eval: false
---

## { .center }

Reproducible <span style="font-weight: normal; font-style: italic">adj.</span>
: (of a measurement, experiment etc) capable of being reproduced at a different time or place and by different people. 

::: footer
[en.wiktionary.org/wiki/reproducible](https://en.wiktionary.org/wiki/reproducible)
:::

::: notes
Have you ever written the perfect program?

Has it still run _unchanged_ 6 months later?

Can your colleagues run it without you?

Just because your analysis is executable, it doesn't mean the results are reproducible.

Data ages. Libraries change. Machines differ. Servers go down. Bits rot. Entropy is inescapable.

This talk explains how to engineer reproducibility drawing on techniques from functional programming and the MLOps movement. The code examples are designed for an R audience but the lessons apply to any language.
:::



## {background-color="#b00400" background-image="images/benevolent-monster.jpg" }

::: { .footer style="color: #FFFFFF" }
NASA (2011) "Benevolent Monster" Solar Flare
:::

::: notes
Physics tells us that energy can't be created or destroyed but that doesn't make it an infinitely enduring resource.

We can only *use* energy when we transform it from one form to another.
:::

## { background-color="#ecd9bd" background-image="images/wattschen-dampfmaschine-cropped.jpg" background-size="auto 80%"}

<!-- ![](images/wattschen-dampfmaschine.jpg){height=100%} -->

::: footer
Schreinmer, R.M. (1912) Deutsches Museum, München

© The Board of Trustees of the Science Museum released under a CC BY-NC-SA 4.0 license at [collection.sciencemuseumgroup.org.uk](https://collection.sciencemuseumgroup.org.uk/objects/co50970/portfolio-of-drawings-etc-used-in-the-reproduction-of-watts-rotative-engine-1788-drawings)
:::

::: notes
We can turn the chemical energy stored in coal into heat by burning it. A fire turns water into steam which expands to drive pistons becoming mechanical energy that we can use. We do work by transforming energy.

But once the water has become steam and expanded it can't expand again unless it is first condensed by cooling.
:::

---


![](images/maxwells-demon.svg)

::: notes
In order to use energy we need order in the universe. An ordered division between hot places and cold ones.

Once we've made use of the difference the heat evens out and now both places are merely warm. We can no longer use that energy.

Though we're not lost energy we have created _entropy_. Entropy is energy that's not available to do work with.

The pictures shows the thought experiment of Maxwell's daemon who opens a door between two vessels letting the faster gas molecules pass and closing it just in time to keep the colder ones separated. In doing so it appears to defy the 2nd law of thermodynamics by reversing entropy.
:::

## { background-image="images/ruins.jpg" }

::: footer
Photo by <a href="https://unsplash.com/@dmey503?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Dan Meyers</a> on <a href="https://unsplash.com/photos/abandoned-industrial-building-interior-Wy0RUh-1DyM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
:::

::: notes
In an isolated system, absent any magical demons, while the amount of energy remains the same, _entropy_ is always increasing. We can create pockets of order but the overall trend is towards disorder. Decay is inevitable.
:::

---

## {background-color="#000000" background-image="images/webb-deep-field.png"}

::: footer
[Advanced Deep Extragalactic Survey](https://webbtelescope.org/contents/media/images/2023/127/01H1Q1CGJD51CDJTK2NHJWD06M) - NASA, ESA, CSA

Brant Robertson (UC Santa Cruz), Ben Johnson (CfA), Sandro Tacchella (Cambridge), Marcia Rieke (University of Arizona), Daniel Eisenstein (CfA)
:::

::: notes
Eventually our universe will reach a state of complete disorder. A uniform cold vacuum with no exploitable differences. The heat death of the universe.

> This is the way the world ends, not with a bang but a whimper.

:::

## { background-color="#000000" background-transition="fade" transition-speed="slow"}

::: notes

Of course we don't generally worry about this cosmic tragedy and I'm not saying that the work we do attempting to bring order to data is futile. Far from it.

This is merely a reminder that we should not have the hubris to expect our programs will work forever while our physical reality tends inexorably towards disorder. No-one, not even the greatest engineers or scientists, are exempt.

We have reason for hope though. Not least because we have a tool that exists beyond the physical constraints of reality...

:::


## { background-color="#000000" .center transition="fade" }

<span style="font-size: 200%">$$f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$</span>

::: notes

The mathematics we use is pure and eternal. Functions are infinitely reproducible. 

Here's the probability density function of the normal distribution.

Pi and Eulers number are real constants. Given the same mean and standard deviation this formula will always plot the same bell-curve and return the same probability density for any given value of x.

This is all very well, but - as beautiful as this formula is - it's not much use on it's own.

:::

---


```{r rnorm-density, echo=T, eval=T}
plot(dnorm, -3, 3, main="The Standard Normal Distribution")
```

::: notes
In practical terms we need to do computations to use that maths and computers are certainly subject to entropy.

Computation sits atop a whole stack of dependencies from your code though packages and programming languages down to machine instructions and the bare metal itself. None of this is permanent.
:::

---

::: {layout-ncol=2}

![](images/leftpad.png)

![](images/dependency.png)
:::

::: {.notes}
Consider the tale of Leftpad... not even a dozen lines of JavaScript was removed from NPM in 2016. It brought front-end development to a screeching halt as build failures cascaded across the web.

:::

::: footer
[theregister.com/2016/03/23/npm_left_pad_chaos/](https://www.theregister.com/2016/03/23/npm_left_pad_chaos/)

CC-BY-NC 2.5 [xkcd.com/2347/](https://xkcd.com/2347/)
:::

## { background-image="images/gallery-404.png"}

::: notes

Consider Gallery 404, a museum of obsolete digital art that was exhibited at the last Berlin Creative Code Stammtisch. None of these art works are reproducible any longer. Server's go down. Domains expire. Adobe flash is long gone. 

The curator explained how some museums nowadays expect to be supplied with working hardware (with backup systems) in order to continue running digital art over the longer term.

:::

## { .center } Functional Reproducibility

::: notes
So, how can we guard against this inevitable bit rot?

How can we design and engineer our code to be reproducible?

It's time to leave the metaphysics and consider some more practical guidance.

As I alluded to earlier, we can turn to maths for help and in code that manifests most directly in terms of functional programming. I'll explain some lessons from that approach then go on to describe ideas from software engineering that have found a home in the data work under the label "MLOps".

Let's start though, with a question...
:::



## Which of the following is not reproducible?

Read from a database

```{r eval=F}
users <- DBI::dbReadTable(connection, "users")
```

Draw a sample at random

```{r eval=F}
coin_tosses <- sample(c("heads","tails"), 10, replace = TRUE)
```

Write to the filesystem

```{r eval=F}
readr::write_csv(data, "output.csv")
```

::: notes
Which of these statements are not reproducible?
:::

# None of them are reproducible!

::: notes
None of them are. This was a trick question!

Reproducibility requires that doing the same thing gives you the same result.

None of those statements - even writing to disk - are reproducible.

Let's explore why and what you can do about it.

We saw the definition at the beginning, but what does it really mean to be reproducible?
:::

## Pure functions are reproducible

```{dot}
//| fig-height: 6
//| eval: true
//| echo: false

digraph D {
  node [style="filled" penwidth=0 fillcolor="#f0f0ff" fontcolor=black fontname="Source Sans Pro"]

  Input -> Function
  Function -> Output
}
```

::: notes
Pure functions are reproducible.

The output of a pure function only depends on it's input. The result doesn't change if it's calculated a second time or by another person - for the same input you always get the same output. You could replace the function call with it's return value in your program. This is known as *referential transparency*. Indeed you could replace the function body with a lookup table that records the relevant output for each input.

A pure function has no side effects. Running it doesn't change the state of the world, the only consequence is the output value it returns.
:::

## Side-effects aren't reproducible

```{dot eval=T}
//| fig-height: 6
//| eval: true
//| echo: false

digraph D {
  node [style="filled" penwidth=0 fillcolor="#f0f0ff" fontcolor=black fontname="Source Sans Pro"]
  { rank=same Function SideEffect }
  
  SideEffect[label="Side Effect"]
  
  Input -> Function
  SideEffect -> Function
  Function -> SideEffect
  Function -> Output
}
```

::: notes
This is all very well in theory but we can't continue piping data around in circles *ad nauseum*. At some point we have to interact with the outside world - read from a database, deploy a website, email a report. In practice we need functions with side-effects.

Side-effects are intended consequences that happen outside of a function or a pipeline's output. We also use the term "side-effect" to refer to causes that exist outside of a function or a pipeline's direct inputs, that is to say *non-local state*.

These side-effects are what make our pipelines useful allowing them to interact with the world. They're also what cause our pipelines to become non-reproducible.

This mightn't be very obvious if it's the first time you're hearing this, so let's look at some examples.
:::

## Non-local state makes functions sensitive to context

```{python eval=T}
counter = 0

def show_count():
  print(f'Count is {counter}')

show_count()
```

Sometime later...
```{python eval=T}
counter += 1

show_count()
```

::: notes
Here we can see a side-effecting `show_count()` function. It implicitly depends on the `counter` variable that is defined in the global scope. This means that it's result depends on the context in which it is run.

Any time you call `show_count()` you could get a different result. It isn't reproducible. If the counter changes, then so does the result.

This function also has no explicit return value. The call to `print` is itself a side-effect. We can't use it in a reproducible pipeline.

Let's refactor this into a pure function.
:::

## Explicit inputs/ outputs let us separate code and context

```{python eval=T}
counter = 0

def describe_count(count):
  return f'Count is {count}'

print(describe_count(counter))
```

Sometime later...
```{python eval=T}
counter += 1

print(describe_count(counter))
```

::: footer
[The Value of Values - Rich Hickey @ JaxConf 2012](https://www.youtube.com/watch?v=-6BsiVyC1kM)
:::

::: notes
We define `describe_count` in terms of the `count` value. Now the dependency on this variable is explicit. Given the same input, this function will always return the same output. It's reproducible. Indeed it now returns an explicit output value reproducibly. The call to `print` happens outside.

The result is essentially the same as before but because it's now reproducible it's much easier to reason about. We've made the dependency on the context explicit. The focus is on the value of the `count`, not the variable and it's place in memory.

This might seem like a toy example but context mutations like this are exactly what's happening when you reassign a variable or use one of Pandas' `inPlace` operations. This problem looks trivial but state mutation is a pernicious source of subtle bugs in notebooks where the execution order and so kernel state may not have evolved linearly.

Mutating state in place may yield savings in computer memory, but it imposes costs on the human capacity to reason about the flow of data through your code.

Pure functions operating on immutable data are reproducible. You lose these guarantees once your pipeline has side-effects and state mutations.
:::

## The context isn't always apparent

```{r eval=T}
toss_coins <- function(n) sample(c("heads","tails"), n, replace = TRUE)

toss_coins(5)
```
Sometime later...
```{r eval=T}
toss_coins(5)
```

::: footer
Pseudo-randomness from a [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister)
:::

::: notes
Some side effects are not obvious.

The `sample` function depends not just on the arguments you pass it but also the state of a random number generator (RNG).

In software the RNG is not truly random but rather a pseudo-random process. It gives highly erratic results but follows [a predictable process](https://en.wikipedia.org/wiki/Mersenne_Twister) if you know the starting state. The starting state is seeded by some source that varies such as the date or a hardware source like `/dev/random` which [collects noise from device drivers](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/char/random.c?id=refs/tags/v3.15.6#n52).

This is a side-effect for our function, causing it's output to differ each time it's run.

Each time I generate *these slides* I get a different set of coin tosses. Although executable, this slide (and a consequence the whole deck) isn't reproducible.
:::

## We can make the context explicit

```{r eval=T}
set.seed(1234) # set state deterministically

toss_coins(5)
```
Sometime later...
```{r eval=T}
set.seed(1234) # reset the state again

toss_coins(5)
```

::: footer
[How hardware seeds `/dev/random`](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/char/random.c?id=refs/tags/v3.15.6#n52)
:::

::: notes
We can make this function call reproducible by fixing the initial state of the random number generator to a constant value with `set.seed`.

This level of purity may be helpful from an engineering perspective (for example this slide has a consistent checksum meaning it can be cached), but it would completely undermine certain analytical procedures. For example in cross-validation where we want to see the test/ train split vary to ensure we're not over-fitting a statistical model.

We might not always want perfect reproducibility. The key is in making that choice consciously.
:::

## I/O is side-effecting

::: incremental
Input is a side-effect

```{r}
df <- readr::read_csv("/home/robin/data-290224-final.csv")
```
:::

::: incremental
Output is a side-effect

```{r}
readr::write_csv(result, "~/results/output.csv")
```
:::

::: notes
A more ubiquitous source of side-effect is I/O (input/ output). This doesn't just apply to third-party APIs or our own database, even the filesystem is non-local state as far as our programs are concerned.

It's not uncommon to see pipelines or notebooks start like this.

The filepath is idiosyncratic and this code won't be reproducible on other people's machines unless they coincidentally have a user called `robin` with this file in the home directory. Indeed this probably won't be reproducible on Robin's machine at a future date unless care is taken to fix that file in place.

More generally, even if the filepath is dependable, there's no guarantee that the content of the CSV file itself won't change.

Even the *writing* of files is not reproducible.

This function alone can't guarantee that the destination directory `~/results` exists.

These problems become all the more apparent with APIs or databases when network interruptions and other external concerns jeopardise the reproducibility of your program.
:::

## Dependency-injection is explicit

```{python}
import pandas as pd

my_conn = snowflake.connector.connect(...)
```

Instead of relying on global state:

```{python}
def get_data():
  return pd.read_sql("SELECT * FROM USERS", my_conn)
  
get_data()
```

We can make dependencies explicit:

```{python}
def get_data(connection):
  return pd.read_sql("SELECT * FROM USERS", connection)

get_data(my_conn)
```

::: notes
The problem isn't that these side-effects exist; they're necessary.

The problem is that when they're implicit they tend to hide dependencies.

We can make them explicit with techniques like dependency injection. The function receives a database connection, for example, as an input instead of relying on it existing in the programs global state.

Now the function declares it's requirements. It draws attention to potential side-effects and forces you to think about them.
:::

## Execution context is explicit

Command-line arguments

```{bash}
python pipeline.py input.csv output.parquet
```

Environment variables

```{bash}
env DB="http://user:pw@localhost:1337" python pipeline.py
```

Configuration data

```{bash}
python pipeline.py configuration.yaml
```

::: footer
[https://12factor.net/](https://12factor.net/)
:::

::: notes
Likewise we can pull side-effects through to the very edges of our pipelines, passing configuration in only at the execution context.

This means the pipeline no longer has to be concerned with coordinating state. We may want some graceful error handling, but the pipeline itself should be context-free and reproducible.

Indeed extending this practice into the execution context itself is what leads to the Infrastructure as Code and DevOps movement where machines and services can themselves be provisioned reproducibly from declarative configuration.
:::

## Functional pipeline,<br />configured context

```{dot eval=T}
//| fig-height: 5
//| eval: true
//| echo: false

digraph D {
  node [style="filled" penwidth=0 fillcolor="#f0f0ff" fontcolor=black fontname="Source Sans Pro"]
  { rank=same Function1 Input }
  { rank=same Function3 Output }
  
  Input -> Function1
  Function1 -> Function2
  Function2 -> Function3
  Function3 -> Output
}
```

::: footer
[Functional core, imperative shell](https://www.destroyallsoftware.com/screencasts/catalog/functional-core-imperative-shell)
:::

::: notes
Pursuing a separation of these concerns - what Gary Bernhart has called [functional core, imperative shell](https://www.destroyallsoftware.com/screencasts/catalog/functional-core-imperative-shell) - leads us to a point where all of the dependencies are captured explicitly and their values gathered together into configuration.

Here we have a pipeline composed of pure-functions with all the necessary side-effects contained to explicitly configured contexts at each end.

This makes it easier to maintain reproducible code. When the code is changed you can see how the pieces fit together and what the consequences of a refactoring are on the rest of the pipeline's code base. When the infrastructure changes you may be lucky enough to only need to change the configuration and not the code at all. This also helps sub-divide the code into modules as each component explicitly declares its requirements making them easier to test in isolation. The preceding examples makes it trivial to pass in a test database connection.

You'll note that this hasn't really fixed the ultimate cause of our problems. As Rick explained, we shouldn't expect to ever be able to fix the rest of the universe in place. The best we can do is hope to contain the unreliable bits, pushing them to the edges so we can carve out a space to pursue reproducibility.
:::

# Lessons from engineering

::: notes
We can go a step further than the above and strive for reproducibility in inputs and outputs, in our interaction with data outside of the pipeline.

In these final slides I'll discuss some lessons the data community can learn from software development. In particular version control and continuous delivery.
:::

## Versions are values over time

![](images/heraclitus.jpg){ .r-stretch style="float:right" }

> It is impossible to step in the same river twice

Heraclitus c.a. 500 BC, possibly apocryphal

::: notes
We've seen that the functional approach leads us to immutability. Steps in your pipeline can only communicate with one another through their arguments and return values. We aren't mutating the objects passing through the pipeline or a global state. The intermediate values are immutable.

But we need to change things to do useful work over time as the external context changes, for example as upstream data sources are updated. How do we cope when the upstream source is mutable? We can use versions to control external change.

Versions identify immutable states of data as it evolves over time.

Heraclitus noted that one can't step in the same river twice. Time marches on. It will be different water and you're a different person.

We have to distinguish the riverbed and the water running through it. We distinguish between an output and instances of it.

Ultimately the version is identified by the values in the data. This becomes cumbersome and we need a more succinct version identifier. Instead we can identity versions by a name formed of two parts. One is a location or label we can use as a reference. The other is the version or state in time. The named output and versioned instances.

Thus we treat data as artifacts, frozen in time.

The parallel here is software releases, not source version control. You can use e.g. `git` to version your data but it's unlikely that line-by-line diff and patches will be very efficient way to store it. You should use version control for the pipeline source code though.

You can deploy files to artifact repositories. Even a simple S3 bucket will let you update a key in place while it records the version history. Some databases support versioning natively or you can create your own snapshots.

This version history let's us retrieve the exact conditions for a pipeline run and reproduce the results.

Be wary of any upstream source that can mutate over time without providing some means of identifying and distinguishing versions. You can of course defend against this to an extent by keeping a track of the data you receive with hashes or recording copies in caches and things like cassettes to record API transactions.

Be a good data citizen and surface versioning information about your own outputs to downstream consumers so that they might ensure reproducibility in their workflow.
:::

## Automation proves reproducibility

![](images/brooms.png){ .r-stretch }

::: notes
If you can run your pipeline locally, even if it's repeatable, it's really just an executable analytical pipeline. It's not demonstrably reproducible until it's running on emphemeral resources in a build system.

A build system runs your workflow recording the versions of the input data, source code, and output. The run pipeline run itself is immutable and identified for posterity with a build number. Done correctly, we don't need to reproduce the pipeline. This contract serves as a guarantee that you'll simply get the same result.

This requires that the process of assembling your dependencies is automated and reproducible itself. No more hunting through Slack or chasing colleagues to find the random excel file that makes the pipeline work. You can't expect a build server to read the readme and figure it out for themselves. Tacit knowledge must be explicitly codified. You can't claim that everything's fine because "it works on my machine". The build server is a shared consensus on configuration. A canonical source you can refer to to see how things work.

Building on an emphemeral stack further enforces the discipline by preventing you from relying on state. I'd argue that the success of containerisation comes as much from virtualisation as from the fact that `Dockerfile`s, for example, automate dependency management down to the operating system level.

There are plenty of choices of Continuous Integration or Continous Deployment tools and increasingly data-specific Workflow systems too.

Don't just say it's reproducible. Prove it!
:::

## How to engineer reproducibility

- Keep the core of your pipeline as pure as possible
- Contain side-effects and make dependencies explicit
- Track (data) version history and automate workflows for reproducible builds

## Functional Reproducibility

[robin@infonomics.ltd.uk]{ style="font-weight: 100; font-size: 1em;"}
